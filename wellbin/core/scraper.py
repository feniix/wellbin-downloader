"""
Wellbin Medical Data Downloader Core Module

Contains the main WellbinMedicalDownloader class for downloading medical data
from the Wellbin platform with support for FhirStudy and DicomStudy types.
"""

import os
import re
import time
from collections import defaultdict
from typing import Any, Optional

import requests
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait


class WellbinMedicalDownloader:
    def __init__(
        self,
        email: str,
        password: str,
        headless: bool = True,
        limit_studies: Optional[int] = None,
        study_types: Optional[list[str]] = None,
        output_dir: str = "downloads",
    ) -> None:
        self.email = email
        self.password = password
        self.base_url = "https://wellbin.co"
        self.login_url = "https://wellbin.co/login"
        self.explorer_url = "https://wellbin.co/explorer"
        self.session = requests.Session()
        self.driver: Optional[webdriver.Chrome] = None
        self.wait: Optional[WebDriverWait[webdriver.Chrome]] = None
        self.headless = headless
        self.limit_studies = limit_studies  # None = all studies, number = limit to that many
        self.study_types = study_types or ["FhirStudy"]  # Default to FhirStudy only
        self.output_dir = output_dir
        self.study_dates: dict[str, str] = {}  # Map study URLs to dates
        self.date_counters: defaultdict[str, int] = defaultdict(int)  # For deduplication per study type

        # Study type configuration
        self.study_config: dict[str, dict[str, str]] = {
            "FhirStudy": {
                "name": "lab",
                "description": "Laboratory Reports",
                "icon": "ğŸ§ª",
                "subdir": "lab_reports",
            },
            "DicomStudy": {
                "name": "imaging",
                "description": "Medical Imaging",
                "icon": "ğŸ©»",
                "subdir": "imaging_reports",
            },
        }

    @staticmethod
    def _is_valid_date(year: int, month: int, day: int) -> bool:
        """Validate that date components form a valid calendar date."""
        if not (1900 <= year <= 2099):
            return False
        if not (1 <= month <= 12):
            return False

        days_in_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]
        # Handle leap years
        if month == 2 and ((year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)):
            max_day = 29
        else:
            max_day = days_in_month[month - 1]

        return 1 <= day <= max_day

    @staticmethod
    def _sanitize_xpath_string(s: str) -> str:
        """Sanitize string for safe use in XPath expressions."""
        if "'" not in s:
            return f"'{s}'"
        if '"' not in s:
            return f'"{s}"'
        # For strings with both quotes, use concat() function
        parts = s.split("'")
        return "concat('" + "', \"'\", '".join(parts) + "')"

    def setup_driver(self) -> None:
        """Setup Chrome driver with appropriate options"""
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        user_agent = (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
        chrome_options.add_argument(f"--user-agent={user_agent}")

        print("ğŸ”§ Setting up Chrome driver...")
        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.implicitly_wait(10)
        self.wait = WebDriverWait(self.driver, 15)
        print("âœ… Chrome driver ready")

    def login(self) -> bool:
        """Login to Wellbin"""
        try:
            print("ğŸ” Logging into Wellbin...")
            self.setup_driver()

            # Navigate to login page
            print(f"ğŸ“ Navigating to: {self.login_url}")
            assert self.driver is not None, "Driver should be initialized"  # nosec
            self.driver.get(self.login_url)
            time.sleep(2)

            # Fill login form
            print("ğŸ“ Filling login credentials...")
            email_field = self.driver.find_element(By.CSS_SELECTOR, "input[type='email']")
            password_field = self.driver.find_element(By.CSS_SELECTOR, "input[type='password']")

            email_field.clear()
            email_field.send_keys(self.email)
            password_field.clear()
            password_field.send_keys(self.password)

            # Submit form
            print("ğŸš€ Submitting login form...")
            submit_button = self.driver.find_element(By.CSS_SELECTOR, "button[type='submit']")
            submit_button.click()

            time.sleep(3)

            # Verify login success
            current_url = self.driver.current_url
            print(f"ğŸ“ After login, current URL: {current_url}")

            if "dashboard" in current_url.lower():
                print("âœ… Login successful!")
                return True
            else:
                print(f"âŒ Login failed. Expected dashboard URL, got: {current_url}")
                return False

        except NoSuchElementException as e:
            print(f"âŒ Login form element not found: {e}")
            print("   Check if login page structure has changed")
            return False
        except Exception as e:
            print(f"âŒ Unexpected error during login: {type(e).__name__}: {e}")
            import traceback

            print(traceback.format_exc())
            return False

    def extract_study_dates_from_explorer(self) -> bool:
        """Extract study dates from the explorer page"""
        try:
            print("ğŸ“… Extracting study dates from explorer page...")
            assert self.driver is not None, "Driver should be initialized"  # nosec
            self.driver.get(self.explorer_url)
            time.sleep(3)

            # Look for date patterns in the page
            # Common date patterns to look for
            date_patterns = [
                r"\b(\d{1,2})[/-](\d{1,2})[/-](\d{4})\b",  # MM/DD/YYYY or DD/MM/YYYY
                r"\b(\d{4})[/-](\d{1,2})[/-](\d{1,2})\b",  # YYYY/MM/DD
                r"\b(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})\b",  # DD Mon YYYY
                r"\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(\d{4})\b",  # Mon DD, YYYY
            ]

            month_map = {
                "Jan": "01",
                "Feb": "02",
                "Mar": "03",
                "Apr": "04",
                "May": "05",
                "Jun": "06",
                "Jul": "07",
                "Aug": "08",
                "Sep": "09",
                "Oct": "10",
                "Nov": "11",
                "Dec": "12",
            }

            # Find all study links and their associated dates
            study_elements = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/study/')]")

            print(f"ğŸ” Found {len(study_elements)} study elements, extracting dates...")

            for element in study_elements:
                try:
                    href = element.get_attribute("href")
                    if not href or "study/" not in href:
                        continue

                    # Get the parent container to look for dates nearby
                    parent_xpath = (
                        "./ancestor::*[contains(@class, 'study') or "
                        "contains(@class, 'card') or "
                        "contains(@class, 'item') or "
                        "contains(@class, 'row')][1]"
                    )
                    parent = element.find_element(By.XPATH, parent_xpath)
                    container_text = parent.text if parent else element.text

                    # Also check siblings and nearby elements
                    try:
                        # Look for date in various nearby elements (using sanitized XPath)
                        href_xpath = self._sanitize_xpath_string(href)
                        date_pattern = "contains(text(), '202') or contains(text(), '201')"
                        nearby_xpath = f"//a[@href={href_xpath}]/ancestor::*[1]//*[{date_pattern}]"
                        nearby_elements = self.driver.find_elements(By.XPATH, nearby_xpath)
                        for nearby in nearby_elements:
                            container_text += " " + nearby.text
                    except Exception as e:  # noqa: S110
                        print(f"    âš ï¸  Could not extract nearby date elements: {type(e).__name__}")

                    # Extract date from the container text
                    study_date = self.parse_date_from_text(container_text, date_patterns, month_map)

                    if study_date:
                        self.study_dates[href] = study_date
                        print(f"  ğŸ“… {href} -> {study_date}")
                    else:
                        # Fallback: extract from URL or use current date
                        study_id_match = re.search(r"/study/([^?]+)", href)
                        if study_id_match:
                            study_id = study_id_match.group(1)
                            # Try to extract date from study ID (some IDs contain timestamps)
                            fallback_date = self.extract_date_from_study_id(study_id)
                            if fallback_date:
                                self.study_dates[href] = fallback_date
                                print(f"  ğŸ“… {href} -> {fallback_date} (from ID)")
                            else:
                                # Use a default date pattern
                                self.study_dates[href] = "20240101"  # Default fallback
                                print(f"  âš ï¸  {href} -> 20240101 (fallback)")

                except Exception as e:
                    print(f"  âŒ Error extracting date for element: {type(e).__name__}: {e}")
                    continue

            print(f"ğŸ“Š Extracted dates for {len(self.study_dates)} studies")
            return True

        except Exception as e:
            print(f"âŒ Error extracting study dates: {type(e).__name__}: {e}")
            import traceback

            print(f"Traceback: {traceback.format_exc()}")
            return False

    def parse_date_from_text(self, text: str, date_patterns: list[str], month_map: dict[str, str]) -> Optional[str]:
        """Parse date from text using various patterns"""
        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    if len(match) == 3:
                        if pattern == date_patterns[0]:  # MM/DD/YYYY or DD/MM/YYYY
                            # Handle both DD/MM/YYYY and MM/DD/YYYY formats
                            part1, part2, year = match
                            part1_int, part2_int, year_int = int(part1), int(part2), int(year)

                            # Try to determine format based on values
                            if part1_int > 12:  # Must be DD/MM/YYYY (day > 12)
                                day, month = part1_int, part2_int
                            elif part2_int > 12:  # Must be MM/DD/YYYY (day > 12)
                                month, day = part1_int, part2_int
                            else:  # Ambiguous - assume DD/MM/YYYY (European format)
                                day, month = part1_int, part2_int

                            # Validate the date
                            if self._is_valid_date(year_int, month, day):
                                return f"{year_int}{month:02d}{day:02d}"

                        elif pattern == date_patterns[1]:  # YYYY/MM/DD
                            year_str, month_str, day_str = match
                            year_int, month_int, day_int = int(year_str), int(month_str), int(day_str)
                            if self._is_valid_date(year_int, month_int, day_int):
                                return f"{year_int}{month_int:02d}{day_int:02d}"

                        elif pattern == date_patterns[2]:  # DD Mon YYYY
                            day_str, month_name, year_str = match
                            day_int, year_int = int(day_str), int(year_str)
                            month_int = int(month_map.get(month_name, "01"))
                            if self._is_valid_date(year_int, month_int, day_int):
                                return f"{year_int}{month_int:02d}{day_int:02d}"

                        elif pattern == date_patterns[3]:  # Mon DD, YYYY
                            month_name, day_str, year_str = match
                            day_int, year_int = int(day_str), int(year_str)
                            month_int = int(month_map.get(month_name, "01"))
                            if self._is_valid_date(year_int, month_int, day_int):
                                return f"{year_int}{month_int:02d}{day_int:02d}"

                except (ValueError, KeyError):
                    continue

        return None

    def extract_date_from_study_id(self, study_id: str) -> Optional[str]:
        """Extract date from study ID if it contains timestamp patterns"""
        # Look for timestamp patterns in study ID
        timestamp_patterns = [
            r"(\d{4})(\d{2})(\d{2})",  # YYYYMMDD
            r"(\d{4})-(\d{2})-(\d{2})",  # YYYY-MM-DD
            r"(\d{4})_(\d{2})_(\d{2})",  # YYYY_MM_DD
        ]

        for pattern in timestamp_patterns:
            match = re.search(pattern, study_id)
            if match:
                year, month, day = match.groups()
                try:
                    year_int, month_int, day_int = int(year), int(month), int(day)
                    # Validate date components using proper calendar validation
                    if self._is_valid_date(year_int, month_int, day_int):
                        return f"{year_int}{month_int:02d}{day_int:02d}"
                except ValueError:
                    continue

        return None

    def extract_dates_for_studies(self, study_links: list[str]) -> None:
        """Extract dates only for specific study links"""
        try:
            # Common date patterns to look for
            date_patterns = [
                r"\b(\d{1,2})[/-](\d{1,2})[/-](\d{4})\b",  # MM/DD/YYYY or DD/MM/YYYY
                r"\b(\d{4})[/-](\d{1,2})[/-](\d{1,2})\b",  # YYYY/MM/DD
                r"\b(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})\b",  # DD Mon YYYY
                r"\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(\d{4})\b",  # Mon DD, YYYY
            ]

            month_map = {
                "Jan": "01",
                "Feb": "02",
                "Mar": "03",
                "Apr": "04",
                "May": "05",
                "Jun": "06",
                "Jul": "07",
                "Aug": "08",
                "Sep": "09",
                "Oct": "10",
                "Nov": "11",
                "Dec": "12",
            }

            assert self.driver is not None, "Driver should be initialized"  # nosec
            # Find study elements that match our filtered links
            for href in study_links:
                try:
                    # Find the specific study element by href (using sanitized XPath)
                    href_xpath = self._sanitize_xpath_string(href)
                    study_elements = self.driver.find_elements(By.XPATH, f"//a[@href={href_xpath}]")

                    for element in study_elements:
                        try:
                            # Get the parent container to look for dates nearby
                            parent_xpath = (
                                "./ancestor::*[contains(@class, 'study') or "
                                "contains(@class, 'card') or "
                                "contains(@class, 'item') or "
                                "contains(@class, 'row')][1]"
                            )
                            parent = element.find_element(By.XPATH, parent_xpath)
                            container_text = parent.text if parent else element.text

                            # Look for dates in the bottom area of study cards
                            try:
                                # Look for date patterns in the entire parent container
                                date_elements = parent.find_elements(By.XPATH, ".//*")
                                for date_elem in date_elements:
                                    elem_text = date_elem.text.strip()
                                    has_date = "/" in elem_text or any(char.isdigit() for char in elem_text)
                                    if elem_text and has_date:
                                        container_text += " " + elem_text

                                # Also check siblings and nearby elements (using sanitized XPath)
                                nearby_xpath = (
                                    f"//a[@href={href_xpath}]/ancestor::*[1]//*["
                                    "contains(text(), '202') or "
                                    "contains(text(), '201') or "
                                    "contains(text(), '/')]"
                                )
                                nearby_elements = self.driver.find_elements(By.XPATH, nearby_xpath)
                                for nearby in nearby_elements:
                                    if nearby.text.strip():
                                        container_text += " " + nearby.text
                            except Exception as e:  # noqa: S110
                                print(f"    âš ï¸  Could not extract date patterns from parent: {type(e).__name__}")

                            # Extract date from the container text
                            study_date = self.parse_date_from_text(container_text, date_patterns, month_map)

                            if study_date:
                                self.study_dates[href] = study_date
                                print(f"  ğŸ“… {href} -> {study_date} (found in text: '{container_text[:100]}...')")
                                break  # Found date for this study, move to next
                            else:
                                # Fallback: extract from URL or use current date
                                study_id_match = re.search(r"/study/([^?]+)", href)
                                if study_id_match:
                                    study_id = study_id_match.group(1)
                                    # Try to extract date from study ID (some IDs contain timestamps)
                                    fallback_date = self.extract_date_from_study_id(study_id)
                                    if fallback_date:
                                        self.study_dates[href] = fallback_date
                                        print(f"  ğŸ“… {href} -> {fallback_date} (from ID)")
                                        break
                                    else:
                                        # Use a default date pattern
                                        self.study_dates[href] = "20240101"  # Default fallback
                                        print(f"  âš ï¸  {href} -> 20240101 (fallback)")
                                        break

                        except Exception as e:
                            print(f"    âŒ Error processing element: {type(e).__name__}: {e}")
                            continue

                    # If no date found for this study, use fallback
                    if href not in self.study_dates:
                        self.study_dates[href] = "20240101"
                        print(f"  âš ï¸  {href} -> 20240101 (no date found)")

                except Exception as e:
                    print(f"  âŒ Error extracting date for {href}: {e}")
                    self.study_dates[href] = "20240101"  # Fallback

            print(f"ğŸ“Š Extracted dates for {len(self.study_dates)} studies")

        except Exception as e:
            print(f"âŒ Error extracting study dates: {e}")

    def extract_date_from_study_page(self, study_url: str) -> str:
        """Extract date from the study page using the item-value report-date class"""
        try:
            assert self.driver is not None, "Driver should be initialized"  # nosec
            # Look for the div with class "item-value report-date"
            date_element = self.driver.find_element(By.CSS_SELECTOR, "div.item-value.report-date")
            date_text = date_element.text.strip()

            if date_text:
                print(f"    ğŸ” Found date text: '{date_text}'")

                # Parse the date using our existing parsing logic
                date_patterns = [
                    r"\b(\d{1,2})[/-](\d{1,2})[/-](\d{4})\b",  # MM/DD/YYYY or DD/MM/YYYY
                    r"\b(\d{4})[/-](\d{1,2})[/-](\d{1,2})\b",  # YYYY/MM/DD
                    r"\b(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})\b",  # DD Mon YYYY
                    r"\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(\d{4})\b",  # Mon DD, YYYY
                ]

                month_map = {
                    "Jan": "01",
                    "Feb": "02",
                    "Mar": "03",
                    "Apr": "04",
                    "May": "05",
                    "Jun": "06",
                    "Jul": "07",
                    "Aug": "08",
                    "Sep": "09",
                    "Oct": "10",
                    "Nov": "11",
                    "Dec": "12",
                }

                parsed_date = self.parse_date_from_text(date_text, date_patterns, month_map)
                if parsed_date:
                    return parsed_date

            # Fallback to study ID extraction if date parsing fails
            study_id_match = re.search(r"/study/([^?]+)", study_url)
            if study_id_match:
                study_id = study_id_match.group(1)
                fallback_date = self.extract_date_from_study_id(study_id)
                if fallback_date:
                    print(f"    âš ï¸  Using fallback date from ID: {fallback_date}")
                    return fallback_date

            print("    âš ï¸  No date found, using default")
            return "20240101"  # Default fallback

        except NoSuchElementException:
            print("    âŒ Could not find div.item-value.report-date element")
            return "20240101"  # Default fallback
        except Exception as e:
            print(f"    âŒ Error extracting date from study page: {e}")
            return "20240101"  # Default fallback

    def get_study_links(self) -> list[str]:
        """Get study links from the explorer page, filtered by study type"""
        try:
            print("ğŸ” Navigating to Explorer to find studies...")
            print(f"ğŸ“ Going to: {self.explorer_url}")
            assert self.driver is not None, "Driver should be initialized"  # nosec
            self.driver.get(self.explorer_url)
            time.sleep(3)

            current_url = self.driver.current_url
            print(f"ğŸ“ Explorer page URL: {current_url}")

            # Look for study links
            study_links: list[str] = []

            # Find all links on the page
            print("ğŸ” Searching for study links...")
            all_links = self.driver.find_elements(By.TAG_NAME, "a")
            print(f"ğŸ“Š Found {len(all_links)} total links on page")

            # Filter by study type FIRST
            print(f"ğŸ¯ Filtering for study types: {', '.join(self.study_types)}")

            for link in all_links:
                try:
                    href = link.get_attribute("href")
                    if href and "study/" in href:
                        # Check if this study type is in our filter
                        study_type_found = False
                        for study_type in self.study_types:
                            if f"type={study_type}" in href:
                                study_type_found = True
                                study_links.append(href)
                                # Extract study type from URL for logging
                                print(f"  âœ… Found {study_type}: {href}")
                                break

                        # Handle "all" study types
                        if not study_type_found and "all" in self.study_types:
                            # Check if it's any of the known study types
                            if any(f"type={st}" in href for st in ["FhirStudy", "DicomStudy"]):
                                study_links.append(href)
                                print(f"  âœ… Found (all types): {href}")

                except Exception as e:
                    print(f"  âŒ Error processing link: {e}")
                    continue

            print(f"ğŸ“Š Found {len(study_links)} matching study links")

            # Apply limit if specified
            if self.limit_studies and len(study_links) > self.limit_studies:
                print(f"ğŸ”¢ Limiting to {self.limit_studies} studies (found {len(study_links)})")
                study_links = study_links[: self.limit_studies]

            # Note: Dates will be extracted when processing individual study pages
            # since they're not available on the explorer page

            return study_links

        except Exception as e:
            print(f"âŒ Error getting study links: {e}")
            return []

    def get_pdf_from_study(self, study_url: str, study_index: int = 1, total_studies: int = 1) -> list[dict[str, Any]]:
        """Get PDF download links from a study page"""
        try:
            # Extract study type from URL
            type_match = re.search(r"type=([^&]+)", study_url)
            study_type = type_match.group(1) if type_match else "Unknown"

            print(f"\n[{study_index}/{total_studies}] ğŸ“„ Processing {study_type} study:")
            print(f"  ğŸ”— URL: {study_url}")

            assert self.driver is not None, "Driver should be initialized"  # nosec
            self.driver.get(study_url)
            time.sleep(1)

            current_url = self.driver.current_url
            print(f"  ğŸ“ Loaded URL: {current_url}")

            # Extract the study date from the study page itself
            study_date = self.extract_date_from_study_page(study_url)
            print(f"  ğŸ“… Study date: {study_date}")

            # Look for the "Descargar estudio" button that points to S3
            print("  ğŸ” Looking for 'Descargar estudio' button...")

            try:
                # Go directly to S3 URL selector since we know it works
                elements = self.driver.find_elements(By.XPATH, "//a[contains(@href, 'wellbin-uploads.s3')]")

                download_element = None
                if elements:
                    # Take the first S3 link found
                    download_element = elements[0]
                    href = download_element.get_attribute("href")
                    text = download_element.text.strip()
                    print(f"  âœ… Found S3 download link: '{text}' -> {href and href[:100]}...")

                if download_element:
                    href = download_element.get_attribute("href")
                    text = download_element.text.strip() or "Download"

                    pdf_info: dict[str, Any] = {
                        "url": href,
                        "text": text,
                        "study_url": study_url,
                        "study_type": study_type,
                        "study_date": study_date,
                    }
                    print(f"  âœ… Found download link: {href and href[:100]}...")
                    return [pdf_info]
                else:
                    # Debug: show all links on the page
                    print("  ğŸ” All links on page:")
                    all_links = self.driver.find_elements(By.TAG_NAME, "a")[:10]  # First 10 links
                    for i, link in enumerate(all_links, 1):
                        href = link.get_attribute("href")
                        text = link.text.strip()
                        print(f"    {i}. '{text}' -> {href[:80] if href else 'No href'}...")

                    print("  âŒ No S3 download link found")
                    return []

            except Exception as e:
                print(f"  âŒ Error finding download link: {e}")
                return []

        except Exception as e:
            print(f"  âŒ Error processing study {study_url}: {e}")
            return []

    def generate_filename(self, study_date: str, study_type: str) -> str:
        """Generate filename with deduplication using format YYYYMMDD-{type}-N.pdf"""
        if study_date == "unknown":
            study_date = "20240101"  # Fallback

        # Get study type configuration
        config = self.study_config.get(study_type, {"name": "unknown"})
        type_name = config["name"]

        # Use study type + date as key for deduplication
        dedup_key = f"{study_type}_{study_date}"
        counter = self.date_counters[dedup_key]
        self.date_counters[dedup_key] += 1

        # Generate filename with counter (0-9)
        filename = f"{study_date}-{type_name}-{counter}.pdf"

        return filename

    def download_pdf(
        self,
        pdf_info: dict[str, Any],
        download_index: int = 1,
        total_downloads: int = 1,
    ) -> Optional[str]:
        """Download a PDF file"""
        try:
            study_date = pdf_info["study_date"]
            print(f"\n[{download_index}/{total_downloads}] ğŸ“¥ Downloading {pdf_info['study_type']} PDF ({study_date}):")
            print(f"  ğŸ“ Description: {pdf_info['text']}")
            print(f"  ğŸ”— URL: {pdf_info['url'][:100]}...")

            # S3 URLs are pre-signed, no authentication needed
            user_agent = (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            )
            headers = {"User-Agent": user_agent}

            print("  ğŸŒ Making download request...")
            response = self.session.get(pdf_info["url"], headers=headers, stream=True)
            print(f"  ğŸ“Š Response status: {response.status_code}")

            if response.status_code != 200:
                print(f"  âŒ Bad response status: {response.status_code}")
                return None

            response.raise_for_status()

            # Generate filename based on study date and type
            filename = self.generate_filename(study_date, pdf_info["study_type"])
            print(f"  ğŸ“ Generated filename: {filename}")

            # Create output directories
            config = self.study_config.get(pdf_info["study_type"], {"subdir": "unknown"})
            output_subdir = os.path.join(self.output_dir, config["subdir"])
            os.makedirs(output_subdir, exist_ok=True)
            filepath = os.path.join(output_subdir, filename)

            # Download file
            print(f"  ğŸ’¾ Saving to: {filepath}")
            file_size = 0
            with open(filepath, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        file_size += len(chunk)

            print("  âœ… Downloaded successfully!")
            print(f"  ğŸ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)")
            return filepath

        except Exception as e:
            print(f"  âŒ Failed to download PDF: {e}")
            import traceback

            print(f"  ğŸ” Traceback: {traceback.format_exc()}")
            return None

    def scrape_studies(self) -> list[dict[str, Any]]:
        """Main method to download studies and download PDFs"""
        downloaded_files: list[dict[str, Any]] = []

        try:
            # Login
            if not self.login():
                print("âŒ Login failed, cannot proceed")
                return downloaded_files

            # Get study links (this also extracts dates)
            study_links = self.get_study_links()
            if not study_links:
                print("âŒ No study links found matching the filter")
                return downloaded_files

            print(f"\nğŸ¯ Processing {len(study_links)} studies...")

            # Process each study
            all_pdf_links: list[dict[str, Any]] = []
            for i, study_url in enumerate(study_links, 1):
                pdf_links = self.get_pdf_from_study(study_url, i, len(study_links))
                for pdf in pdf_links:
                    pdf["study_index"] = i
                all_pdf_links.extend(pdf_links)

                # Brief delay between studies
                if i < len(study_links):
                    time.sleep(0.5)

            if not all_pdf_links:
                print("âŒ No PDF links found in any studies")
                return downloaded_files

            print(f"\nğŸ“Š Found {len(all_pdf_links)} total PDF files to download")
            print("=" * 60)

            # Download all PDFs
            for i, pdf_info in enumerate(all_pdf_links, 1):
                filepath = self.download_pdf(pdf_info, i, len(all_pdf_links))
                if filepath:
                    downloaded_files.append(
                        {
                            "local_path": filepath,
                            "original_url": pdf_info["url"],
                            "study_url": pdf_info["study_url"],
                            "study_type": pdf_info["study_type"],
                            "study_date": pdf_info["study_date"],
                            "description": pdf_info["text"],
                            "study_index": pdf_info["study_index"],
                        }
                    )

                # Brief delay between downloads
                if i < len(all_pdf_links):
                    time.sleep(0.2)

            return downloaded_files

        except Exception as e:
            print(f"âŒ Error during download: {type(e).__name__}: {e}")
            import traceback

            print(f"ğŸ” Traceback: {traceback.format_exc()}")
            return downloaded_files
        finally:
            # Clean up resources
            try:
                if self.driver:
                    print("ğŸ”’ Closing browser...")
                    self.driver.quit()
            except Exception as e:
                print(f"âš ï¸  Error closing browser: {type(e).__name__}: {e}")

            # Ensure session is closed
            try:
                if self.session:
                    self.session.close()
            except Exception as e:
                print(f"âš ï¸  Error closing session: {type(e).__name__}: {e}")
